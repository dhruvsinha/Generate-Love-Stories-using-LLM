# Generate Love Stories using Large Language Models

CHRISTOPHER T. KANG and DHRUV SINHA, The University of Chicago, USA

## Introduction

Large Language Models (LLMs) are lauded for their ability to consume massive volumes of text. This ability is pertinent when
consuming corpora containing long-form text, e.g., books, scientific papers, or when generating long-form text. However, specific
genres — like poetry — are defined by their brevity. In this work, we train GPT2 Neo via prompt engineering and fine-tuning on the New
York Times’ Tiny Love Stories. Human evaluation suggests that prompt engineered stories are indistinguishable from human-written
stories. We explore what this implies for the capabilities of LLMs in concise, emotion-rich regimes. 

## Task and Methodology

In our text generation task, we provide the model with a real NYT Tiny Love Stories title and seek to generate a realistic
love story within 100 words. To achieve this generation, we proceed with the project in three phases:

* Gather data: We scrape the title and text of Tiny Love Stories, then perform a qualitative analysis of the
obtained stories.
* Training: We test two training strategies, prompt engineering and fine-tuning, and analyze their performance
when generating text.
* Evaluation: We compare the generated and actual text, then conduct human studies to test the indistinguishability of LLM-generated text.

In this summary document, we will briefly describe the training and evaluation part of this project. Please refer to the paper attached in the repository to read more about each component of this project. 

## Training
Because our task is text generation, we choose to use the GPT family of models. In particular, the GPT Neo models are
well-known and have been optimized. We consider two of the newer models, **GPT2-Neo 2.7B and GPT2-NeoX 20B**. Using these two models, we test two strategies to generate content: **Prompt Engineering and Fine Tuning**. The results from fine-tuned model were incomprehensible. We have discussed the potential reasons in the paper. Here, we have discussed the Prompt Engineering approach briefly:

Our prompt engineering approach is intuitive " we provide the following prompt structure to the model:

Title: [Example title]

Story: [Example story]


Title: [Test title]

Story:

During generation, we randomly select an example title/story pair from the training dataset. We then requested the
model generate between 100-200 tokens and truncated any excess text.
While other applications would find this prompt approach challenging due to the context limit, the text length
restriction imposed by the dataset makes prompt engineering feasible. It is challenging to think of examples where 100
word stories would exceed 1024 tokens, even at a character-level tokenization.
When using the 20B model, our generation consistently generated syntactically correct stories. Furthermore, the
length of stories generated often mimicked the input story’s length, thus implicitly approximating the NYT length
requirement.

## Results

We got some outstanding results. The stories generated by LLM were indistinguishable. We encourage the readers to take these short surveys and see for themselves the performance of large language model. 


* [Form 2](https://docs.google.com/forms/d/e/1FAIpQLSdSxA3BVIqFFdDWP6hX0LDDwsLDI5uSqS_8Q7OqKnpPZr9U9w/viewform?usp=share_link): In this form, we received 4 responses with an average score of 2.75.
* [Form 3](https://docs.google.com/forms/d/e/1FAIpQLScqKud-pk28Z5cFf4Y_luLJ3ptr9yGUxNHrD3RF0HNKXhz6fw/viewform?usp=share_link): In this form, we received only one response with a score of 5.
* [Form 4](https://docs.google.com/forms/d/e/1FAIpQLScWT10QdEu8e62zzMnkn-E0IdQ3C8NFiPSTKQypxFyPcgW9gQ/viewform?usp=share_link): In this form, we received 8 responses with an average score of 2.88.

If we collate the responses from all the forms we received responses on, the average score was 50% for 13 responses.
This means that an evaluator, on average, correctly answered 3 out of 6 questions.If we only consider the stories written by humans, 53.84% of them were correctly predicted. This means that on all
the human-written stories in the survey, only around 54% of them were correctly guessed as human-written. Model-generated stories followed a similar trend- around 46.15% of them were correctly predicted. **While our sample size is
small, this supports the hypothesis that stories generated by humans and LLMs are indistinguishable.**

## Discussion

Our primary aim, indistinguishability, is supported when analyzing our quantitative results ( 50% accuracy). When
studying which stories were typically judged as human-generated, we noticed three core attributes:
1. **Subtle emotional content**: Tiny Love Stories are fundamentally emotional — they aim to invoke an emotional
response in the reader. Evaluators preferred stories with an emotional appeal
2. **Information persistence**: While content was important, we also observed the importance of inter-sentence
syntax. Because the 20B model largely was correct syntactically for individual sentences, evaluators instead
relied on the overall composition of passages.
3. **Relevance to provided title**: Unsurprisingly, the alignment of the story with respect to the title was a key
piece of information, even in spite of TCG evaluation method

For example:

***Who’s There?***

*My 88-year-old mother looks at the screen, squinting. "Who’s there, you say?" My brother explains that
the boxes frame her other children. Three thousand miles away, in Chicago, I wait for my 97-year-old
father to sit down. Zoom, coronavirus, Lima, Chicago, Florida and dementia collide on our screens. I
usually travel to Peru every other month to care for my parents. Without international flights, I feel as
lost as my mother. I shoo away questions that start with "What if. . . ?" Today, when my mother asks,
"Who’s there?" I say, "Tu hija." ("Your daughter.")*

All evaluators correctly guessed that this story was human-generated. In this story, we hypothesize the use of the
phrase ‘Tu Hija’ led reviewers to judge this as human-written. This could be because ‘Tu Hija’ is a Spanish phrase
which is consistent with the context that her parents live in Peru, leading to a sense that information is consistently
and gradually revealed. 

As a second example:

***Music From Myanmar***

*The music is like nothing he has experienced before, but that is the kind of guy Taw Oo is. Originally
from Myanmar, he spent nearly two decades as a rock star in Chicago before he found himself back
in Myanmar, in a country ravaged by human rights abuses and war. Now he runs a popular pop-up
concert in Yangon, his homeland, where he performs songs full of hope for a peaceful future, even
though no one there can hear them. There’s a reason he keeps the..*

Though this story was generated by the LLM, most evaluators guessed that it was written by a human. We hypothesize
that this is explainable by the presence of a general emotional appeal and continuity between sentences (e.g., Taw Oo
seems like a reasonable name from Myanmar; Myanmar has faced human rights abuses)

Large Language Models (LLMs) are lauded for their ability to consume massive volumes of text. This ability is pertinent when
consuming corpora containing long-form text, e.g., books, scientific papers, or when generating long-form text. However, specific
genres — like poetry — are defined by their brevity. In this work, we train GPT2 Neo via prompt engineering and fine-tuning on the New
York Times’ Tiny Love Stories. Human evaluation suggests that prompt engineered stories are indistinguishable from human-written
stories. We explore what this implies for the capabilities of LLMs in concise, emotion-rich regimes. 

EMAIL FOR CODE!
